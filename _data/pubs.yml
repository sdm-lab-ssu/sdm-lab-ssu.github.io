# When making a new entry, copy the commented out entry, paste it, uncomment, and fill out the fields.
# If any of the fields are confusing look at previous examples for guidance.
#- abs: null
#  authors: null
#  award: null
#  bib: null
#  img: null
#  links: {}
#  short_id: null
#  site: null
#  title: null
#  venue: null
#  video_embed: null
#  tags: null
# Current tags for research areas, see the research areas page for more details:
# deform_obj_manip, 3D_afford_obj_manip, multimodal, rl_algs, auto_driving, active_perception, self_sup_rob

- abs: Vision-Language Models (VLMs) have revolutionized artificial intelligence
    and robotics due to their commonsense reasoning capabilities. In robotic
    manipulation, VLMs are used primarily as high-level planners, but recent
    work has also studied their lower-level reasoning ability, which refers to
    making decisions about precise robot movements. However, the community
    currently lacks a clear and common benchmark that can evaluate how well VLMs
    can aid low-level reasoning in robotics. Consequently, we propose a novel
    benchmark, ManipBench, to evaluate the low-level robot manipulation
    reasoning capabilities of VLMs across various dimensions, including how well
    they understand object-object interactions and deformable object
    manipulation. We extensively test 33 representative VLMs across 10 model
    families on our benchmark, including variants to test different model sizes.
    Our evaluation shows that the performance of VLMs significantly varies
    across tasks, and there is a strong correlation between this performance and
    trends in our real-world manipulation tasks. It also shows that there
    remains a significant gap between these models and human-level understanding. 
  authors: "Enyu Zhao*, Vedant Raval*, Hejia Zhang*, Jiageng Mao, Zeyu Shangguan, Stefanos Nikolaidis, Yue Wang, Daniel Seita"
  award: null
  bib: "@inproceedings{zhao2025ManipBench,\n
    \ title={{ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation}},\n
    \ author={Enyu Zhao and Vedant Raval and Hejia Zhang and Jiageng Mao and Zeyu Shangguan and Stefanos Nikolaidis and Yue Wang and Daniel Seita},\n
    \ booktitle={Conference on Robot Learning (CoRL)},\n
    \ Year={2025}\n}"
  img: ../img/papers/ManipBench_CoRL2025.gif
  links:
    '[arXiv]': https://arxiv.org/abs/2505.09698
  short_id: 2025_ManipBench
  site: https://manipbench.github.io/ 
  title: "ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation"
  venue: Conference on Robot Learning (CoRL), 2025. 
  video_embed: null
  tags:
    - benchmark 
    - vlm 

- abs: Learning bimanual manipulation is challenging due to its high
    dimensionality and tight coordination required between two arms. Eye-in-hand
    imitation learning, which uses wrist-mounted cameras, simplifies perception
    by focusing on task-relevant views. However, collecting diverse
    demonstrations remains costly, motivating the need for scalable data
    augmentation. While prior work has explored visual augmentation in
    single-arm settings, extending these approaches to bimanual manipulation
    requires generating viewpoint-consistent observations across both arms and
    producing corresponding action labels that are both valid and feasible. In
    this work, we propose Diffusion for COordinated Dual-arm Data Augmentation
    (D-CODA), a method for offline data augmentation tailored to eye-in-hand
    bimanual imitation learning that trains a diffusion model to synthesize
    novel, viewpoint-consistent wrist-camera images for both arms while
    simultaneously generating joint-space action labels. It employs constrained
    optimization to ensure that augmented states involving gripper-to-object
    contacts adhere to constraints suitable for bimanual coordination. We
    evaluate D-CODA on 5 simulated and 3 real-world tasks. Our results across
    2250 simulation trials and 300 real-world trials demonstrate that it
    outperforms baselines and ablations, showing its potential for scalable data
    augmentation in eye-in-hand bimanual manipulation. 
  authors: "I-Chun Arthur Liu, Jason Chen, Gaurav Sukhatme, Daniel Seita"
  award: null
  bib: "@inproceedings{liu2025DCODA,\n
    \ title={{D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation}},\n
    \ author={I-Chun Arthur Liu and Jason Chen and Gaurav Sukhatme and Daniel Seita},\n
    \ booktitle={Conference on Robot Learning (CoRL)},\n
    \ Year={2025}\n}"
  img: ../img/papers/DCODA_CoRL2025.gif
  links:
    '[arXiv]': https://arxiv.org/abs/2505.04860 
  short_id: 2025_DCODA
  site: https://dcodaaug.github.io/D-CODA/ 
  title: "D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation"
  venue: Conference on Robot Learning (CoRL), 2025. 
  video_embed: null
  tags:
    - dexterous_hands

